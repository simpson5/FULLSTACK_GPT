### Data Loaders와 Splitters

#### Document Loaders 개요
Document Loader는 다양한 소스의 데이터를 LangChain에서 활용 가능한 형태로 변환하는 도구입니다.

##### 주요 Document Loaders
- 기본 포맷: CSV, HTML, JSON, Markdown, Text 등
- 통합 지원: GitHub, Figma, Facebook, Slack, Twitter 등
- 문서 포맷: PDF, DOCX, PowerPoint 등

##### UnstructuredFileLoader
```python
from langchain.document_loaders import UnstructuredFileLoader

# 다양한 파일 형식을 단일 인터페이스로 로드
loader = UnstructuredFileLoader("./files/chapter_one.docx")
documents = loader.load()
```
- 다양한 파일 형식(PDF, DOCX, TXT 등) 지원
- 자동으로 필요한 의존성 패키지 설치
- 통합된 인터페이스로 간편한 사용

#### Document Splitters

##### 문서 분할의 필요성
1. 큰 문서를 관리 가능한 크기로 분할
2. 관련된 컨텍스트만 선별적으로 활용
3. 토큰 제한을 고려한 청크 사이즈 관리

##### RecursiveCharacterTextSplitter
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,      # 청크 최대 크기
    chunk_overlap=100,   # 청크 간 중복 크기
    separators=["\n"]    # 분할 기준
)

# loader와 splitter 통합 사용
split_docs = loader.load_and_split(text_splitter=splitter)
```

##### 주요 설정 매개변수
- **chunk_size**: 각 청크의 최대 크기 (문자 수)
- **chunk_overlap**: 청크 간 중복되는 부분의 크기
- **separators**: 문서 분할 기준 (예: 줄바꿈, 문장 기호 등)

##### 분할 전략
1. **청크 크기 설정**
   - 모델의 컨텍스트 윈도우 고려
   - 의미 단위 보존을 위한 적절한 크기 선택

2. **중복 영역 활용**
   - 문맥 유지를 위한 청크 간 연결성 확보
   - 의미 단위가 분리되는 것을 방지

3. **구분자 기반 분할**
   - 문단, 문장 등 자연스러운 단위로 분할
   - 의미 단위 보존을 위한 구분자 선택

#### 실제 사용 시 고려사항
- 문서의 특성에 따른 적절한 splitter 선택
- 청크 크기와 중복 영역의 균형
- 모델의 토큰 제한 고려
- 의미 단위 보존을 위한 분할 전략 수립

#### 다음 단계
- 임베딩(Embedding) 처리
- OpenAI 토큰 계산 함수 활용
- Vector Store 구성 