### LLM 사용량 측정과 모델 직렬화

- **LLM 사용량(비용) 측정**
  - get_openai_callback을 with문과 함께 사용해, LLM 호출 시 토큰 사용량과 비용을 측정
  - usage 객체에서 total_cost, total_tokens, prompt_tokens, completion_tokens 등 다양한 정보 확인 가능
  - 여러 번의 LLM 호출도 블록 내에서 합산해 측정 가능

- **모델 직렬화(Serialization)**
  - LLM(OpenAI 등) 설정을 json 파일로 저장(.save), 필요 시 불러오기(.load_llm)
  - 모델 파라미터(예: max_tokens, model명 등)까지 저장/복원 가능
  - 협업, 재현성, 배포 등에 유용

- **실습 코드 요약**
  - with get_openai_callback() as usage: 블록 내에서 LLM 여러 번 호출, usage로 비용/토큰 확인
  - chat.save("model.json")으로 모델 설정 저장, load_llm("model.json")으로 불러오기

- **활용 포인트**
  - 비용 관리, 사용량 모니터링, 모델 설정의 손쉬운 저장/복원에 유용

---
참조:  
- https://python.langchain.com/docs/modules/model_io/callbacks#tracking-openai-api-usage
- https://python.langchain.com/docs/modules/model_io/llms/serialization 