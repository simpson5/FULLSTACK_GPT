### LLM Caching 개념과 실습

- **LLM Caching(캐싱) 개념**
  - 동일한 질문에 대해 LLM이 동일한 응답을 반환하도록, 응답을 캐시에 저장해 재사용
  - 비용 절감, 응답 속도 향상, 반복 호출 시 효율적
  - LangChain은 다양한 캐시 백엔드 지원: InMemoryCache, SQLiteCache, RedisCache 등
  - set_llm_cache로 캐시 백엔드 지정, set_debug로 디버그 모드 활성화 가능

- **주요 사용 흐름**
  1. set_llm_cache로 캐시 백엔드(InMemory, SQLite 등) 설정
  2. LLM 호출 시 동일 입력이면 캐시된 응답 반환(속도↑, 비용↓)
  3. set_debug(True)로 프롬프트/응답 로그 확인 가능
  4. 캐시 파일(DB)은 재시작 시에도 유지(메모리 캐시는 휘발성)

- **실습 코드 요약**
  - set_llm_cache(SQLiteCache("cache.db"))로 DB 캐시 설정
  - chat.predict("How do you make italian pasta") → 첫 호출은 LLM, 두 번째 호출은 캐시에서 즉시 반환
  - set_debug(True)로 프롬프트/응답 로그 확인

- **활용 포인트**
  - 반복 질문이 많은 챗봇, FAQ 등에서 비용 절감 및 응답 속도 개선에 매우 효과적
  - 다양한 캐시 백엔드로 확장 가능

---
참조:  
- https://python.langchain.com/docs/modules/model_io/caching 