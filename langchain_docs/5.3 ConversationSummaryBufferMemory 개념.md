### ConversationSummaryBufferMemory 개념

- **개요**
  - ConversationBufferMemory와 ConversationSummaryMemory의 혼합
  - 최근 메시지는 그대로 저장, 오래된 메시지는 요약
  - 토큰 제한을 초과하면 자동으로 오래된 메시지 요약

- **주요 특징**
  - `max_token_limit` 파라미터로 토큰 수 제한
  - 토큰 제한 초과 시:
    1. 오래된 메시지들을 LLM으로 요약
    2. 요약본은 시스템 메시지로 저장
    3. 최근 메시지는 그대로 유지
  - 전체 문맥 유지와 토큰 효율성의 균형

- **구현 예시**
  ```python
  from langchain.memory import ConversationSummaryBufferMemory
  from langchain.chat_models import ChatOpenAI
  
  # LLM 설정
  llm = ChatOpenAI(temperature=0.1)
  
  # 메모리 초기화 (최대 150 토큰)
  memory = ConversationSummaryBufferMemory(
      llm=llm,
      max_token_limit=150,
      return_messages=True
  )
  
  # 대화 추가
  memory.save_context(
      {"input": "Hi, I'm Nicolas"},
      {"output": "Hello Nicolas!"}
  )
  
  # 여러 메시지 추가 후 토큰 제한 초과 시
  # 이전 메시지들이 자동으로 요약됨
  memory.save_context(
      {"input": "How far is Korea from Argentina?"},
      {"output": "I don't know the exact distance"}
  )
  ```

- **메모리 구조**
  ```python
  # 메모리 구조 예시
  {
      "messages": [
          SystemMessage(content="이전 대화 요약..."),  # 요약된 이전 대화
          HumanMessage(content="최근 질문1"),         # 최근 메시지들은
          AIMessage(content="최근 답변1"),           # 그대로 저장
          HumanMessage(content="최근 질문2"),
          AIMessage(content="최근 답변2")
      ]
  }
  ```

- **장단점**
  - 장점
    - 전체 문맥 유지
    - 토큰 수 효율적 관리
    - 최근 대화는 상세하게 유지
  - 단점
    - LLM 호출 비용 발생
    - 구현 복잡도 증가
    - 요약 시점 관리 필요

- **활용 시 주의사항**
  - `max_token_limit` 설정이 중요
    - 너무 작으면 잦은 요약 발생
    - 너무 크면 토큰 비용 증가
  - 요약 품질 모니터링
    - 중요 정보 보존 확인
    - 요약-최근 메시지 간 일관성
  - 비용 관리
    - 요약 빈도 조절
    - LLM 호출 최적화

---
참조:
- https://python.langchain.com/docs/modules/memory/types/summary_buffer 