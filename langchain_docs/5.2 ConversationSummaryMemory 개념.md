### ConversationSummaryMemory 개념

- **개요**
  - LLM을 사용해 대화 내용을 요약하여 저장하는 메모리
  - 전체 대화의 문맥을 유지하면서 토큰 수를 줄이는 방식
  - 초기에는 토큰 수가 많지만, 장기적으로는 효율적

- **동작 방식**
  - 새로운 대화가 추가될 때마다 LLM이 전체 대화를 요약
  - 요약본을 시스템 메시지로 저장
  - 예시:
    - 입력: "Hi, I'm Nicolas. I live in South Korea"
    - 출력: "The human introduces themselves as Nicolas and mentions they live in South Korea"

- **구현 예시**
  ```python
  from langchain.memory import ConversationSummaryMemory
  from langchain.chat_models import ChatOpenAI
  
  # LLM 설정 (요약에 사용)
  llm = ChatOpenAI(temperature=0.1)
  
  # 메모리 초기화
  memory = ConversationSummaryMemory(
      llm=llm,
      return_messages=True
  )
  
  # 대화 추가
  memory.save_context(
      {"input": "Hi I'm Nicolas, I live in South Korea"},
      {"output": "Wow that's so cool!"}
  )
  
  memory.save_context(
      {"input": "South Korea is so pretty"},
      {"output": "I wish I could go"}
  )
  
  # 메모리 확인 - 요약된 형태로 저장됨
  memory.load_memory_variables({})
  ```

- **장단점**
  - 장점
    - 전체 대화 문맥 유지
    - 장기 대화에서 토큰 효율성
    - 중요 정보 보존
  - 단점
    - 초기 토큰 수가 많음 (요약이 원문보다 길 수 있음)
    - LLM 호출 비용 발생
    - 요약 과정에서 세부 정보 손실 가능

- **활용 시 주의사항**
  - LLM 선택이 중요
    - 요약 품질에 직접적 영향
    - temperature 값은 낮게 설정 (0.1~0.3)
  - 비용 고려
    - 매 대화마다 LLM 호출 필요
    - 초기에는 비용이 더 많이 발생
  - 요약 품질 모니터링
    - 중요 정보가 누락되지 않는지 확인
    - 필요시 프롬프트 튜닝

---
참조:
- https://python.langchain.com/docs/modules/memory/types/summary
- https://python.langchain.com/docs/modules/memory/types/summary_buffer 